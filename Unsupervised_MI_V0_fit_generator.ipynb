{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ae998b-1fa9-40ac-88f5-0294d36c8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Libraries for VGG \n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as io\n",
    "from skimage.transform import resize, rotate\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d00d9a-127d-4ca5-a70d-4672ad33dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A to E are standard VGG backbones\n",
    "# F was customized for IIC\n",
    "# G is experimental\n",
    "cfg = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',512, 512, 512, 512, 'M'],\n",
    "    'F': [64, 'M', 128, 'M', 256, 'M', 512],\n",
    "    'G': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'A'],\n",
    "}\n",
    "\n",
    "class VGG:\n",
    "    def __init__(self, cfg, input_shape=(28, 28, 1)): #(80, 1000, 1)\n",
    "        \"\"\"VGG network model creator to be used as backbone\n",
    "            feature extractor\n",
    "\n",
    "        Arguments:\n",
    "            cfg (dict): Summarizes the network configuration\n",
    "            input_shape (list): Input image dims\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.input_shape = input_shape\n",
    "        self._model = None\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Model builder uses a helper function\n",
    "            make_layers to read the config dict and\n",
    "            create a VGG network model\n",
    "        \"\"\"\n",
    "        inputs = Input(shape=self.input_shape, name='x')\n",
    "        x = VGG.make_layers(self.cfg, inputs)\n",
    "        self._model = Model(inputs, x, name='VGG')\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @staticmethod\n",
    "    def make_layers(cfg,\n",
    "                    inputs, \n",
    "                    batch_norm=True, \n",
    "                    in_channels=1):\n",
    "        \"\"\"Helper function to ease the creation of VGG\n",
    "            network model\n",
    "\n",
    "        Arguments:\n",
    "            cfg (dict): Summarizes the network layer \n",
    "                configuration\n",
    "            inputs (tensor): Input from previous layer\n",
    "            batch_norm (Bool): Whether to use batch norm\n",
    "                between Conv2D and ReLU\n",
    "            in_channel (int): Number of input channels\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        for layer in cfg:\n",
    "            if layer == 'M':\n",
    "                x = MaxPooling2D()(x)\n",
    "            elif layer == 'A':\n",
    "                x = AveragePooling2D(pool_size=3)(x)\n",
    "            else:\n",
    "                x = Conv2D(layer,\n",
    "                           kernel_size=3,\n",
    "                           padding='same',\n",
    "                           kernel_initializer='he_normal'\n",
    "                           )(x)\n",
    "                if batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = Activation('relu')(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e976d87a-d593-496f-a62e-cc6dbb53cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VGG\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 3, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,553,664\n",
      "Trainable params: 1,551,744\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build backbone\n",
    "backbone = VGG(cfg['F'])\n",
    "backbone.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a88766-0452-4572-8c94-290f05e7d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mnist #Dataset to use\n",
    "epochs = 3 #Number of epochs to train\n",
    "batch_size = 512 #Train batch size\n",
    "heads = 1 #Number of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03872ab1-3f4d-4e37-931b-fccc532e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence): # Multi-threaded data generator. Each thread reads a batch of images and performs image transformation such that the image class is unaffected\n",
    "    def __init__(self, shuffle=True):\n",
    "        self.shuffle = shuffle # shuffle (Bool): Whether to shuffle the dataset before sampling or not\n",
    "        self._dataset()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self): # Number of batches per epoch\n",
    "        return int(np.floor(len(self.indexes) / batch_size))\n",
    "\n",
    "    def __getitem__(self, index): # Image sample Indexes for the current batch\n",
    "        start_index = index * batch_size\n",
    "        end_index = (index+1) * batch_size\n",
    "        return self.__data_generation(start_index, end_index)\n",
    "\n",
    "    def _dataset(self): # Load dataset and normalize it\n",
    "        (self.data, _), (_, _) = dataset.load_data()\n",
    "        self.n_channels = 1\n",
    "        self.input_shape = [self.data.shape[1], self.data.shape[2], self.n_channels]\n",
    "        self.n_labels = 10\n",
    "        self.indexes = [i for i in range(self.data.shape[0])]\n",
    "        \n",
    "        # reshape and normalize input images\n",
    "        new_shape = [-1, self.data.shape[1], self.data.shape[2], self.n_channels]\n",
    "        self.data = np.reshape(self.data, new_shape)\n",
    "        self.data = self.data.astype('float32') / 255\n",
    "\n",
    "    def on_epoch_end(self): # If opted, shuffle dataset after each epoch\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def random_rotate(self,image,deg=20,target_shape=(28, 28, 1)): # Random image rotation\n",
    "        angle = np.random.randint(-deg, deg) # deg (int): Degrees of rotation\n",
    "        image = rotate(image, angle)\n",
    "        image = resize(image, target_shape)\n",
    "        return image\n",
    "    \n",
    "    def __data_generation(self, start_index, end_index): # Data generation algorithm. The method generates a batch of pair of images (original image X and transformed imaged Xbar). \n",
    "        x = self.data[self.indexes[start_index : end_index]] # Given an array of images.  the start index to retrieve a batch, the end index to retrieve a batch\n",
    "        target_shape = (x.shape[0], *self.input_shape)\n",
    "        x1 = np.zeros(target_shape)\n",
    "        x2 = np.zeros(target_shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            image = x[i]\n",
    "            image_bar = self.random_rotate(image,target_shape=target_shape[1:])\n",
    "            x1[i] = image\n",
    "            x2[i] = image_bar\n",
    "        x_train = np.concatenate([x1, x2], axis=0) # for IIC, we are mostly interested in paired images X and Xbar = G(X)\n",
    "\n",
    "        y = np.zeros(len(x_train))\n",
    "        return x_train, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e411c8-dbf0-4493-9eb0-67eb0953122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "VGG (Functional)             (None, 3, 3, 512)         1553664   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "z_head0 (Dense)              (None, 10)                46090     \n",
      "=================================================================\n",
      "Total params: 1,599,754\n",
      "Trainable params: 1,597,834\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobkritikos/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def mi_loss(y_true, y_pred): # Mutual information loss computed from the joint distribution matrix and the marginals\n",
    "    # y_true (tensor): Not used since this is unsupervised learning\n",
    "    # y_pred (tensor): stack of softmax predictions the latent vectors (Z and Zbar)\n",
    "    size = batch_size\n",
    "    n_labels = y_pred.shape[-1]\n",
    "    # lower half is Z\n",
    "    Z = y_pred[0: size, :]\n",
    "    Z = K.expand_dims(Z, axis=2)\n",
    "    # upper half is Zbar\n",
    "    Zbar = y_pred[size: y_pred.shape[0], :]\n",
    "    Zbar = K.expand_dims(Zbar, axis=1)\n",
    "    # compute joint distribution (Eq 10.3.2 & .3)\n",
    "    P = K.batch_dot(Z, Zbar)\n",
    "    P = K.sum(P, axis=0)\n",
    "    # enforce symmetric joint distribution (Eq 10.3.4)\n",
    "    P = (P + K.transpose(P)) / 2.0\n",
    "    # normalization of total probability to 1.0\n",
    "    P = P / K.sum(P)\n",
    "    # marginal distributions (Eq 10.3.5 & .6)\n",
    "    Pi = K.expand_dims(K.sum(P, axis=1), axis=1)\n",
    "    Pj = K.expand_dims(K.sum(P, axis=0), axis=0)\n",
    "    Pi = K.repeat_elements(Pi, rep=n_labels, axis=1)\n",
    "    Pj = K.repeat_elements(Pj, rep=n_labels, axis=0)\n",
    "    P = K.clip(P, K.epsilon(), np.finfo(float).max)\n",
    "    Pi = K.clip(Pi, K.epsilon(), np.finfo(float).max)\n",
    "    Pj = K.clip(Pj, K.epsilon(), np.finfo(float).max)\n",
    "    # negative MI loss (Eq 10.3.7)\n",
    "    neg_mi = K.sum((P * (K.log(Pi) + K.log(Pj) - K.log(P))))\n",
    "    # each head contribute 1/n_heads to the total loss\n",
    "    return neg_mi/heads\n",
    "\n",
    "train_gen = DataGenerator(shuffle=True)\n",
    "n_labels = train_gen.n_labels\n",
    "inputs = Input(shape=train_gen.input_shape, name='x') # Build the n_heads of the IIC model\n",
    "my_backbone = backbone.model\n",
    "x = my_backbone(inputs)\n",
    "x = Flatten()(x)\n",
    "outputs = [] # number of output heads\n",
    "for i in range(heads):\n",
    "    name = \"z_head%d\" % i\n",
    "    outputs.append(Dense(n_labels,\n",
    "                         activation='softmax',\n",
    "                         name=name)(x))\n",
    "my_model = Model(inputs, outputs, name='encoder')\n",
    "my_model.compile(optimizer=Adam(lr=1e-3), loss=mi_loss)\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba125e54-ee4a-41d1-ac81-1da4389c7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-load test data for evaluation\n",
    "(_, _), (x_test, y_test) = dataset.load_data()\n",
    "x_test = np.reshape(x_test,[-1, x_test.shape[1], x_test.shape[2], 1])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_eval = np.zeros([x_test.shape[0], *train_gen.input_shape])\n",
    "for i in range(x_eval.shape[0]):\n",
    "    x_eval[i] = x_test[i]\n",
    "x_test = x_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5f46f6-aa50-4546-8404-1689b0abc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobkritikos/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 506s 4s/step - loss: -1.2706\n",
      "Head 0 accuracy: 52.13%\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 470s 4s/step - loss: -1.6256\n",
      "Head 0 accuracy: 59.69%, Old best accuracy: 52.13%\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "117/117 [==============================] - 465s 4s/step - loss: -1.6863\n",
      "Head 0 accuracy: 63.82%, Old best accuracy: 59.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcd60f78100>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "def lr_schedule(epoch): # Simple learning rate scheduler // Argument: epoch (int): Which epoch \n",
    "    lr = 1e-3\n",
    "    power = epoch // 400\n",
    "    lr *= 0.8**power\n",
    "    return lr\n",
    "\n",
    "def unsupervised_labels(y, yp, n_classes, n_clusters): # Linear assignment algorithm\n",
    "    assert n_classes == n_clusters # Arguments: y (tensor): Ground truth labels // yp (tensor): Predicted clusters // n_classes (int): Number of classes // n_clusters (int): Number of clusters\n",
    "    C = np.zeros([n_clusters, n_classes]) # initialize count matrix\n",
    "    for i in range(len(y)): # populate count matrix\n",
    "        C[int(yp[i]), int(y[i])] += 1\n",
    "    row, col = linear_sum_assignment(-C) # optimal permutation using Hungarian Algo the higher the count, the lower the cost so we use -C for linear assignment\n",
    "    accuracy = C[row, col].sum() / C.sum() # compute accuracy\n",
    "    return accuracy * 100\n",
    "\n",
    "class AccuracyCallback(Callback): # Callback to compute the accuracy every epoch by calling the eval() method.\n",
    "    def __init__(self):\n",
    "        super(AccuracyCallback, self).__init__()\n",
    "        self.general_accuracy = 0\n",
    "    def on_epoch_end(self, epoch, logs=None): # Evaluate the accuracy of the current model weights\n",
    "        y_pred = my_model.predict(x_test)\n",
    "        for head in range(heads): # accuracy per head\n",
    "            if heads == 1:\n",
    "                y_head = y_pred\n",
    "            else:\n",
    "                y_head = y_pred[head]\n",
    "            y_head = np.argmax(y_head, axis=1)\n",
    "\n",
    "            accuracy = unsupervised_labels(list(y_test),list(y_head),n_labels,n_labels)\n",
    "            info = \"Head %d accuracy: %0.2f%%\"\n",
    "            if self.general_accuracy > 0:\n",
    "                info += \", Old best accuracy: %0.2f%%\"\n",
    "                data = (head, accuracy, self.general_accuracy)\n",
    "            else:\n",
    "                data = (head, accuracy)\n",
    "            print(info % data)\n",
    "            if accuracy > self.general_accuracy:\n",
    "                self.general_accuracy = accuracy # if accuracy improves during training, save the model weights on a file\n",
    "                    \n",
    "# Train function uses the data generator, accuracy computation, and learning rate scheduler callbacks        \n",
    "my_model.fit_generator(generator=train_gen,\n",
    "                            use_multiprocessing=False,\n",
    "                            epochs=epochs,\n",
    "                            callbacks=[AccuracyCallback(), LearningRateScheduler(lr_schedule,verbose=1)],\n",
    "                            workers=4,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5deda428-2480-439d-bc6c-96adcb1df488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 0 accuracy: 63.82\n"
     ]
    }
   ],
   "source": [
    "# Load eval data for evaluation\n",
    "(_, _), (x_test, y_test) = dataset.load_data()\n",
    "image_size = x_test.shape[1]\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_eval = np.zeros([x_test.shape[0], *train_gen.input_shape])\n",
    "for i in range(x_eval.shape[0]):\n",
    "    x_eval[i] = x_test[i]\n",
    "    \n",
    "y_pred = my_model.predict(x_eval)\n",
    "for head in range(heads): # accuracy per head\n",
    "    if heads == 1:\n",
    "        y_head = y_pred\n",
    "    else:\n",
    "        y_head = y_pred[head]\n",
    "    y_head = np.argmax(y_head, axis=1)\n",
    "\n",
    "    accuracy = unsupervised_labels(list(y_test),list(y_head),n_labels,n_labels)\n",
    "    print(\"Head\", head,\"accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
